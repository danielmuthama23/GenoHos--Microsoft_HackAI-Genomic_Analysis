from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
import pandas as pd
from tenacity import retry, wait_random_exponential, stop_after_attempt
from openai import AzureOpenAI
from notebookutils import mssparkutils
import time
import logging
from config import Config

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("BiospecimenRAG")

# Initialize Spark with enhanced configuration
spark = SparkSession.builder \
    .appName("PDC_Biospecimen_Production") \
    .config("spark.sql.execution.arrow.pyspark.enabled", "true") \
    .config("spark.sql.shuffle.partitions", "8") \
    .getOrCreate()

# Initialize OpenAI client with retry logic
@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6))
def init_openai_client():
    return AzureOpenAI(
        azure_endpoint=Config.OPENAI_ENDPOINT,
        api_key=Config.OPENAI_KEY,
        api_version=Config.OPENAI_API_VERSION
    )

client = init_openai_client()

class BiospecimenQuerySystem:
    def __init__(self):
        self.initialized = False
        self.embeddings_cache = {}
        self.kusto_token = mssparkutils.credentials.getToken(Config.KUSTO_URI)
        self.initialize_system()
        
    def initialize_system(self):
        logger.info("Starting system initialization")
        start_time = time.time()
        
        try:
            self.gold_df = self._load_or_transform_data()
            
            if not self._check_embeddings_exist():
                logger.info("Generating and storing embeddings")
                embeddings_df = self._prepare_embeddings()
                self._store_embeddings(embeddings_df)
                
            self.initialized = True
            logger.info(f"System initialized in {time.time()-start_time:.2f} seconds")
            
        except Exception as e:
            logger.error(f"Initialization failed: {str(e)}")
            raise

    def _load_or_transform_data(self):
        if spark.catalog.tableExists("gold_pdc_biospecimen"):
            logger.info("Loading existing gold data")
            return spark.table("gold_pdc_biospecimen")
        
        logger.info("Transforming raw data to gold standard")
        df = spark.read.format("csv").option("header", "true").load(Config.DATA_PATH)
        
        # Validation and transformation
        required_columns = ["Sample_Type", "Primary_Site", "Aliquot_ID"]
        for col in required_columns:
            if col not in df.columns:
                raise ValueError(f"Missing required column: {col}")
        
        df = df.select([col(c).alias(c.replace(" ", "_").replace("-", "_")) for c in df.columns])
        
        numeric_cols = ["Aliquot_Quantity", "Aliquot_Volume", "Concentration"]
        for col in numeric_cols:
            if col in df.columns:
                df = df.withColumn(col, col(col).cast(DoubleType()))
        
        df.write.format("delta").mode("overwrite").saveAsTable("gold_pdc_biospecimen")
        return df

    def _check_embeddings_exist(self):
        try:
            test_df = spark.read \
                .format("com.microsoft.kusto.spark.synapse.datasource") \
                .option("kustoCluster", Config.KUSTO_URI) \
                .option("kustoDatabase", Config.KUSTO_DB) \
                .option("accessToken", self.kusto_token) \
                .option("kustoQuery", f"{Config.KUSTO_TABLE} | count") \
                .load()
            return test_df.collect()[0][0] > 0
        except Exception as e:
            logger.warning(f"Embeddings check failed: {str(e)}")
            return False

    def _prepare_embeddings(self, batch_size=100):
        logger.info("Preparing embeddings")
        pdf = self.gold_df.limit(5000).toPandas()
        
        def process_batch(batch):
            try:
                return client.embeddings.create(
                    input=[text.replace("\n", " ") for text in batch],
                    model=Config.OPENAI_EMBEDDING_DEPLOYMENT
                ).data
            except Exception as e:
                logger.error(f"Failed batch: {str(e)}")
                return None
        
        embeddings = []
        for i in range(0, len(pdf), batch_size):
            batch = pdf.iloc[i:i+batch_size]
            batch_embeddings = process_batch(batch['document_text'].tolist())
            if batch_embeddings:
                embeddings.extend([e.embedding for e in batch_embeddings])
                logger.info(f"Processed batch {i//batch_size + 1}/{(len(pdf)//batch_size)+1}")
        
        pdf['embedding'] = embeddings[:len(pdf)]
        return spark.createDataFrame(pdf[['Aliquot_ID', 'document_text', 'Sample_Type', 'Primary_Site', 'embedding']])

    def _store_embeddings(self, df):
        logger.info("Storing embeddings in Eventhouse")
        try:
            df.write \
                .format("com.microsoft.kusto.spark.synapse.datasource") \
                .option("kustoCluster", Config.KUSTO_URI) \
                .option("kustoDatabase", Config.KUSTO_DB) \
                .option("kustoTable", Config.KUSTO_TABLE) \
                .option("accessToken", self.kusto_token) \
                .mode("Append") \
                .save()
        except Exception as e:
            logger.error(f"Failed to store embeddings: {str(e)}")
            raise

    def query(self, question, nr_of_answers=3):
        if not self.initialized:
            return {"error": "System initializing", "status": 503}
        
        start_time = time.time()
        
        try:
            cache_key = question.lower().strip()
            if cache_key in self.embeddings_cache:
                logger.info("Returning cached result")
                return self.embeddings_cache[cache_key]
            
            embedding = self.generate_embeddings(question)
            
            kusto_query = f"""
            {Config.KUSTO_TABLE}
            | extend similarity = series_cosine_similarity(
                dynamic({str(embedding)}), 
                embedding
              )
            | top {nr_of_answers} by similarity desc
            | project 
                content=document_text, 
                metadata=pack(
                    'sample_type', Sample_Type,
                    'primary_site', Primary_Site,
                    'aliquot_id', Aliquot_ID
                ),
                similarity
            """
            
            results = spark.read \
                .format("com.microsoft.kusto.spark.synapse.datasource") \
                .option("kustoCluster", Config.KUSTO_URI) \
                .option("kustoDatabase", Config.KUSTO_DB) \
                .option("accessToken", self.kusto_token) \
                .option("kustoQuery", kusto_query) \
                .load() \
                .collect()
            
            response = {
                "answer": self._generate_llm_response(question, results),
                "sources": [row.asDict() for row in results],
                "processing_time": time.time() - start_time,
                "status": 200
            }
            
            self.embeddings_cache[cache_key] = response
            return response
            
        except Exception as e:
            logger.error(f"Query failed: {str(e)}")
            return {"error": str(e), "status": 500}

    def _generate_llm_response(self, question, results):
        context = "\n".join(
            f"Record {i+1} (Similarity: {row['similarity']:.2f}):\n{row['content']}"
            for i, row in enumerate(results)
        )
        
        prompt = f"""You are a biomedical research assistant analyzing biospecimen data.
        
        Question: {question}
        
        Relevant Records:
        {context}
        
        Provide:
        1. A 1-2 sentence answer
        2. Key characteristics of matching samples
        3. Confidence assessment based on similarity scores"""
        
        response = client.chat.completions.create(
            model=Config.OPENAI_GPT4_DEPLOYMENT,
            messages=[
                {"role": "system", "content": prompt},
                {"role": "user", "content": question}
            ],
            temperature=0.2,
            max_tokens=250
        )
        return response.choices[0].message.content

# Initialize system
system = BiospecimenQuerySystem()

# Interactive mode
if __name__ == "__main__":
    while True:
        try:
            user_input = input("\nEnter question (or 'exit'): ")
            if user_input.lower() in ['exit', 'quit']:
                break
                
            if user_input.strip():
                result = system.query(user_input)
                print("\nAnswer:", result.get('answer', 'No answer generated'))
                print("\nSources:")
                for src in result.get('sources', [])[:3]:
                    print(f"- {src['content']} (Similarity: {src['similarity']:.2f})")
                
        except KeyboardInterrupt:
            break
        except Exception as e:
            print(f"Error: {str(e)}")